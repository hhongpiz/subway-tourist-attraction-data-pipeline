{"is_success": "Fail", "type": "weather", "err_msg": "An error occurred while calling o146.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 26) (localhost executor driver): java.sql.BatchUpdateException: ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:733)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\tSuppressed: java.sql.SQLIntegrityConstraintViolationException: ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 1400, Position : 102, Sql = INSERT INTO WEATHER (\"DAY\",\"TIME\",\"RAIN\",\"HUMN\",\"SNOW\",\"SKY\",\"ONDO\",\"WINDD\",\"WINDS\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ), OriginalSql = INSERT INTO WEATHER (\"DAY\",\"TIME\",\"RAIN\",\"HUMN\",\"SNOW\",\"SKY\",\"ONDO\",\"WINDD\",\"WINDS\") VALUES (?,?,?,?,?,?,?,?,?), Error Msg = ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\tat oracle.jdbc.driver.OraclePreparedStatement.generateBatchUpdateException(OraclePreparedStatement.java:10323)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10090)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeLargeBatch(OraclePreparedStatement.java:9975)\n\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatch(OraclePreparedStatement.java:9932)\n\tat oracle.jdbc.driver.OracleStatementWrapper.executeBatch(OracleStatementWrapper.java:262)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:733)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n\tSuppressed: java.sql.SQLIntegrityConstraintViolationException: ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:628)\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:562)\n\t\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1145)\n\t\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:726)\n\t\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:291)\n\t\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:492)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:148)\n\t\tat oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1038)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeForRowsWithTimeout(OraclePreparedStatement.java:9892)\n\t\tat oracle.jdbc.driver.OraclePreparedStatement.executeBatchWithoutQueue(OraclePreparedStatement.java:10069)\n\t\t... 17 more\n\tCaused by: Error : 1400, Position : 102, Sql = INSERT INTO WEATHER (\"DAY\",\"TIME\",\"RAIN\",\"HUMN\",\"SNOW\",\"SKY\",\"ONDO\",\"WINDD\",\"WINDS\") VALUES (:1 ,:2 ,:3 ,:4 ,:5 ,:6 ,:7 ,:8 ,:9 ), OriginalSql = INSERT INTO WEATHER (\"DAY\",\"TIME\",\"RAIN\",\"HUMN\",\"SNOW\",\"SKY\",\"ONDO\",\"WINDD\",\"WINDS\") VALUES (?,?,?,?,?,?,?,?,?), Error Msg = ORA-01400: cannot insert NULL into (\"ADMIN\".\"WEATHER\".\"RAIN\")\n\n\t\tat oracle.jdbc.driver.T4CTTIoer11.processError(T4CTTIoer11.java:632)\n\t\t... 26 more\n"}
{"is_success": "Fail", "type": "weather", "err_msg": "local variable 'weather' referenced before assignment"}
{"is_success": "Fail", "type": "weather", "err_msg": "cannot resolve 'tm' given input columns: [weather.DAY, weather.TIME, weather.dc10Tca, weather.hm, weather.hr3Fhsc, weather.rn, weather.ta, weather.wd, weather.ws]; line 1 pos 7;\n'Project ['tm, CASE WHEN isnull(rn#100) THEN cast(0 as float) ELSE rn#100 END AS RAIN#117, hm#101 AS HUMN#118, CASE WHEN isnull(hr3Fhsc#102) THEN cast(0 as double) ELSE round((cast(hr3Fhsc#102 as double) / cast(3 as double)), 1) END AS SNOW#119, CASE WHEN (dc10Tca#103 <= cast(5 as float)) THEN 1 WHEN (dc10Tca#103 <= cast(8 as float)) THEN 3 ELSE 4 END AS SKY#120, ta#104 AS ONDO#121, FLOOR(CheckOverflow((promote_precision(cast(CheckOverflow((promote_precision(cast(cast(wd#105 as decimal(10,0)) as decimal(13,2))) + promote_precision(cast(CheckOverflow((promote_precision(cast(22.5 as decimal(3,1))) * promote_precision(cast(0.5 as decimal(3,1)))), DecimalType(5,2), true) as decimal(13,2)))), DecimalType(13,2), true) as decimal(13,2))) / promote_precision(cast(22.5 as decimal(13,2)))), DecimalType(18,6), true)) AS WINDD#122, ws#106 AS WINDS#123]\n+- SubqueryAlias weather\n   +- View (`weather`, [DAY#107,TIME#99,rn#100,hm#101,hr3Fhsc#102,dc10Tca#103,ta#104,wd#105,ws#106])\n      +- Sort [time#99 ASC NULLS FIRST], true\n         +- Filter ((time#99 >= 10:00) AND (time#99 <= 22:00))\n            +- Project [cast(substring(tm#52, 1, 10) as date) AS DAY#107, substring(tm#52, 12, 17) AS TIME#99, cast(rn#42 as float) AS rn#100, cast(hm#28 as int) AS hm#101, cast(hr3Fhsc#30 as float) AS hr3Fhsc#102, cast(dc10Tca#24 as float) AS dc10Tca#103, cast(ta#49 as float) AS ta#104, cast(wd#56 as int) AS wd#105, cast(ws#58 as float) AS ws#106]\n               +- LogicalRDD [clfmAbbrCd#22, dc10LmcsCa#23, dc10Tca#24, dmstMtphNo#25, dsnw#26, gndSttCd#27, hm#28, hmQcflg#29, hr3Fhsc#30, icsr#31, lcsCh#32, m005Te#33, m01Te#34, m02Te#35, m03Te#36, pa#37, paQcflg#38, ps#39, psQcflg#40, pv#41, rn#42, rnQcflg#43, rnum#44, ss#45, ... 14 more fields], false\n"}
{"is_success": "Fail", "type": "weather", "err_msg": "cannot resolve 'tm' given input columns: [weather.DAY, weather.TIME, weather.dc10Tca, weather.hm, weather.hr3Fhsc, weather.rn, weather.ta, weather.wd, weather.ws]; line 1 pos 7;\n'Project ['tm, CASE WHEN isnull(rn#100) THEN cast(0 as float) ELSE rn#100 END AS RAIN#117, hm#101 AS HUMN#118, CASE WHEN isnull(hr3Fhsc#102) THEN cast(0 as double) ELSE round((cast(hr3Fhsc#102 as double) / cast(3 as double)), 1) END AS SNOW#119, CASE WHEN (dc10Tca#103 <= cast(5 as float)) THEN 1 WHEN (dc10Tca#103 <= cast(8 as float)) THEN 3 ELSE 4 END AS SKY#120, ta#104 AS ONDO#121, FLOOR(CheckOverflow((promote_precision(cast(CheckOverflow((promote_precision(cast(cast(wd#105 as decimal(10,0)) as decimal(13,2))) + promote_precision(cast(CheckOverflow((promote_precision(cast(22.5 as decimal(3,1))) * promote_precision(cast(0.5 as decimal(3,1)))), DecimalType(5,2), true) as decimal(13,2)))), DecimalType(13,2), true) as decimal(13,2))) / promote_precision(cast(22.5 as decimal(13,2)))), DecimalType(18,6), true)) AS WINDD#122, ws#106 AS WINDS#123]\n+- SubqueryAlias weather\n   +- View (`weather`, [DAY#107,TIME#99,rn#100,hm#101,hr3Fhsc#102,dc10Tca#103,ta#104,wd#105,ws#106])\n      +- Sort [time#99 ASC NULLS FIRST], true\n         +- Filter ((time#99 >= 10:00) AND (time#99 <= 22:00))\n            +- Project [cast(substring(tm#52, 1, 10) as date) AS DAY#107, substring(tm#52, 12, 17) AS TIME#99, cast(rn#42 as float) AS rn#100, cast(hm#28 as int) AS hm#101, cast(hr3Fhsc#30 as float) AS hr3Fhsc#102, cast(dc10Tca#24 as float) AS dc10Tca#103, cast(ta#49 as float) AS ta#104, cast(wd#56 as int) AS wd#105, cast(ws#58 as float) AS ws#106]\n               +- LogicalRDD [clfmAbbrCd#22, dc10LmcsCa#23, dc10Tca#24, dmstMtphNo#25, dsnw#26, gndSttCd#27, hm#28, hmQcflg#29, hr3Fhsc#30, icsr#31, lcsCh#32, m005Te#33, m01Te#34, m02Te#35, m03Te#36, pa#37, paQcflg#38, ps#39, psQcflg#40, pv#41, rn#42, rnQcflg#43, rnum#44, ss#45, ... 14 more fields], false\n"}
{"is_success": "Fail", "type": "weather", "err_msg": "cannot resolve 'tm' given input columns: [weather.DAY, weather.TIME, weather.dc10Tca, weather.hm, weather.hr3Fhsc, weather.rn, weather.ta, weather.wd, weather.ws]; line 1 pos 7;\n'Project ['tm, CASE WHEN isnull(rn#100) THEN cast(0 as float) ELSE rn#100 END AS RAIN#117, hm#101 AS HUMN#118, CASE WHEN isnull(hr3Fhsc#102) THEN cast(0 as double) ELSE round((cast(hr3Fhsc#102 as double) / cast(3 as double)), 1) END AS SNOW#119, CASE WHEN (dc10Tca#103 <= cast(5 as float)) THEN 1 WHEN (dc10Tca#103 <= cast(8 as float)) THEN 3 ELSE 4 END AS SKY#120, ta#104 AS ONDO#121, FLOOR(CheckOverflow((promote_precision(cast(CheckOverflow((promote_precision(cast(cast(wd#105 as decimal(10,0)) as decimal(13,2))) + promote_precision(cast(CheckOverflow((promote_precision(cast(22.5 as decimal(3,1))) * promote_precision(cast(0.5 as decimal(3,1)))), DecimalType(5,2), true) as decimal(13,2)))), DecimalType(13,2), true) as decimal(13,2))) / promote_precision(cast(22.5 as decimal(13,2)))), DecimalType(18,6), true)) AS WINDD#122, ws#106 AS WINDS#123]\n+- SubqueryAlias weather\n   +- View (`weather`, [DAY#107,TIME#99,rn#100,hm#101,hr3Fhsc#102,dc10Tca#103,ta#104,wd#105,ws#106])\n      +- Sort [time#99 ASC NULLS FIRST], true\n         +- Filter ((time#99 >= 10:00) AND (time#99 <= 22:00))\n            +- Project [cast(substring(tm#52, 1, 10) as date) AS DAY#107, substring(tm#52, 12, 17) AS TIME#99, cast(rn#42 as float) AS rn#100, cast(hm#28 as int) AS hm#101, cast(hr3Fhsc#30 as float) AS hr3Fhsc#102, cast(dc10Tca#24 as float) AS dc10Tca#103, cast(ta#49 as float) AS ta#104, cast(wd#56 as int) AS wd#105, cast(ws#58 as float) AS ws#106]\n               +- LogicalRDD [clfmAbbrCd#22, dc10LmcsCa#23, dc10Tca#24, dmstMtphNo#25, dsnw#26, gndSttCd#27, hm#28, hmQcflg#29, hr3Fhsc#30, icsr#31, lcsCh#32, m005Te#33, m01Te#34, m02Te#35, m03Te#36, pa#37, paQcflg#38, ps#39, psQcflg#40, pv#41, rn#42, rnQcflg#43, rnum#44, ss#45, ... 14 more fields], false\n"}
{"is_success": "Fail", "type": "weather", "err_msg": "cannot resolve 'tm' given input columns: [weather.DAY, weather.TIME, weather.dc10Tca, weather.hm, weather.hr3Fhsc, weather.rn, weather.ta, weather.wd, weather.ws]; line 1 pos 7;\n'Project ['tm, CASE WHEN isnull(rn#100) THEN cast(0 as float) ELSE rn#100 END AS RAIN#117, hm#101 AS HUMN#118, CASE WHEN isnull(hr3Fhsc#102) THEN 0 ELSE cast(round((cast(hr3Fhsc#102 as double) / cast(3 as double)), 1) as string) END AS SNOW#119, CASE WHEN (dc10Tca#103 <= cast(5 as float)) THEN 1 WHEN (dc10Tca#103 <= cast(8 as float)) THEN 3 ELSE 4 END AS SKY#120, ta#104 AS ONDO#121, FLOOR(CheckOverflow((promote_precision(cast(CheckOverflow((promote_precision(cast(cast(wd#105 as decimal(10,0)) as decimal(13,2))) + promote_precision(cast(CheckOverflow((promote_precision(cast(22.5 as decimal(3,1))) * promote_precision(cast(0.5 as decimal(3,1)))), DecimalType(5,2), true) as decimal(13,2)))), DecimalType(13,2), true) as decimal(13,2))) / promote_precision(cast(22.5 as decimal(13,2)))), DecimalType(18,6), true)) AS WINDD#122, ws#106 AS WINDS#123]\n+- SubqueryAlias weather\n   +- View (`weather`, [DAY#107,TIME#99,rn#100,hm#101,hr3Fhsc#102,dc10Tca#103,ta#104,wd#105,ws#106])\n      +- Sort [time#99 ASC NULLS FIRST], true\n         +- Filter ((time#99 >= 10:00) AND (time#99 <= 22:00))\n            +- Project [cast(substring(tm#52, 1, 10) as date) AS DAY#107, substring(tm#52, 12, 17) AS TIME#99, cast(rn#42 as float) AS rn#100, cast(hm#28 as int) AS hm#101, cast(hr3Fhsc#30 as float) AS hr3Fhsc#102, cast(dc10Tca#24 as float) AS dc10Tca#103, cast(ta#49 as float) AS ta#104, cast(wd#56 as int) AS wd#105, cast(ws#58 as float) AS ws#106]\n               +- LogicalRDD [clfmAbbrCd#22, dc10LmcsCa#23, dc10Tca#24, dmstMtphNo#25, dsnw#26, gndSttCd#27, hm#28, hmQcflg#29, hr3Fhsc#30, icsr#31, lcsCh#32, m005Te#33, m01Te#34, m02Te#35, m03Te#36, pa#37, paQcflg#38, ps#39, psQcflg#40, pv#41, rn#42, rnQcflg#43, rnum#44, ss#45, ... 14 more fields], false\n"}
