{"is_success": "Fail", "type": "weather_now", "std_day": "20221026", "params": {"ServiceKey": "lWLaCFJasDo6vpnFordk0ZVBBDk0eu0yL%2BKUjEF56K%2Bw78w4lsKg7BJANNWeWbL2mzl72Q4LfFyIygL5qCGGkA%3D%3D", "pageNo": "1", "numOfRows": "1000", "dataType": "JSON", "base_date": "20221026", "base_time": "0800", "nx": "60", "ny": "127"}, "err_msg": "'startDt'"}
{"is_success": "Fail", "type": "weather", "err_msg": "Path does not exist: hdfs://localhost:9000/final_data/weather/weather_20221026.json"}
{"is_success": "Fail", "type": "weather", "err_msg": "name 'weather' is not defined"}
{"is_success": "Fail", "type": "weather", "err_msg": "cannot resolve 'corrupt_record' given input columns: [_corrupt_record];\n'Project ['corrupt_record]\n+- Relation [_corrupt_record#0] json\n"}
{"is_success": "Fail", "type": "weather", "err_msg": "\nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      "}
{"is_success": "Fail", "type": "weather", "err_msg": "\nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      "}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'PCP'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "could not open socket: [\"tried to connect to ('127.0.0.1', 33643), but an error occurred: [Errno 104] Connection reset by peer\"]"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "If using all scalar values, you must pass an index"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "condition should be string or Column"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "unsupported operand type(s) for &: 'bool' and 'str'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "values should be a numeric type."}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'DataFrame' object has no attribute 'to_dataframe'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "cannot resolve 'fcstDate' given input columns: [PCP, POP, PTY, REH, SKY, SNO, TMP, TMX, UUU, VEC, VVV, WAV, WSD];\n'Project [PCP#117, POP#119, PTY#121, REH#123, SKY#125, SNO#127, TMP#129, TMX#131, UUU#133, VEC#135, VVV#137, WAV#139, WSD#141, 'fcstDate AS TIME#215]\n+- Project [PCP#117, POP#119, PTY#121, REH#123, SKY#125, SNO#127, TMP#129, TMX#131, UUU#133, VEC#135, VVV#137, WAV#139, WSD#141]\n   +- Project [fcstDate#25, fcstTime#26, __index_level_3__#78, PCP#117, POP#119, PTY#121, REH#123, SKY#125, SNO#127, TMP#129, TMX#131, UUU#133, VEC#135, VVV#137, WAV#139, WSD#141, monotonically_increasing_id() AS __natural_order__#158L]\n      +- Aggregate [fcstDate#25, fcstTime#26, __index_level_3__#78], [fcstDate#25, fcstTime#26, __index_level_3__#78, first(if ((category#24 <=> cast(PCP as string))) __none__#79 else cast(null as string), true) AS PCP#117, first(if ((category#24 <=> cast(POP as string))) __none__#79 else cast(null as string), true) AS POP#119, first(if ((category#24 <=> cast(PTY as string))) __none__#79 else cast(null as string), true) AS PTY#121, first(if ((category#24 <=> cast(REH as string))) __none__#79 else cast(null as string), true) AS REH#123, first(if ((category#24 <=> cast(SKY as string))) __none__#79 else cast(null as string), true) AS SKY#125, first(if ((category#24 <=> cast(SNO as string))) __none__#79 else cast(null as string), true) AS SNO#127, first(if ((category#24 <=> cast(TMP as string))) __none__#79 else cast(null as string), true) AS TMP#129, first(if ((category#24 <=> cast(TMX as string))) __none__#79 else cast(null as string), true) AS TMX#131, first(if ((category#24 <=> cast(UUU as string))) __none__#79 else cast(null as string), true) AS UUU#133, first(if ((category#24 <=> cast(VEC as string))) __none__#79 else cast(null as string), true) AS VEC#135, first(if ((category#24 <=> cast(VVV as string))) __none__#79 else cast(null as string), true) AS VVV#137, first(if ((category#24 <=> cast(WAV as string))) __none__#79 else cast(null as string), true) AS WAV#139, first(if ((category#24 <=> cast(WSD as string))) __none__#79 else cast(null as string), true) AS WSD#141]\n         +- Project [fcstDate#25, fcstTime#26, category#24, __index_level_3__#78, __none__#79, __natural_order__#85L]\n            +- Project [fcstDate#25, fcstTime#26, category#24, __index_level_3__#78, __none__#79, monotonically_increasing_id() AS __natural_order__#85L]\n               +- Project [fcstDate#25, fcstTime#26, category#24, pairs#70.__index_level_3__ AS __index_level_3__#78, pairs#70.__none__ AS __none__#79]\n                  +- Project [__index_level_0__#43L, fcstDate#25, fcstTime#26, category#24, fcstValue#27, __natural_order__#51L, pairs#70]\n                     +- Generate explode(array(struct(__index_level_3__, fcstValue, __none__, fcstValue#27))), false, [pairs#70]\n                        +- Project [__index_level_0__#43L, fcstDate#25, fcstTime#26, category#24, fcstValue#27, monotonically_increasing_id() AS __natural_order__#51L]\n                           +- Project [__index_level_0__#43L, fcstDate#25, fcstTime#26, category#24, fcstValue#27]\n                              +- Project [fcstDate#25, fcstTime#26, category#24, fcstValue#27, _w0#44L, _we0#45, (cast(_we0#45 as bigint) - cast(1 as bigint)) AS __index_level_0__#43L]\n                                 +- Window [row_number() windowspecdefinition(_w0#44L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _we0#45], [_w0#44L ASC NULLS FIRST]\n                                    +- Project [fcstDate#25, fcstTime#26, category#24, fcstValue#27, monotonically_increasing_id() AS _w0#44L]\n                                       +- Filter ((fcstTime#26 >= 1000) AND (fcstTime#26 <= 2200))\n                                          +- Project [fcstDate#25, fcstTime#26, category#24, fcstValue#27]\n                                             +- LogicalRDD [baseDate#22, baseTime#23, category#24, fcstDate#25, fcstTime#26, fcstValue#27, nx#28L, ny#29L], false\n"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "col should be Column"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "withColumn() missing 1 required positional argument: 'col'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "unstack() takes 1 positional argument but 2 were given"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'fcstDate'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'fcstDate'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "The method `pd.Index.__iter__()` is not implemented. If you want to collect your data as an NumPy array, use 'to_numpy()' instead."}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'MultiIndex' object is not callable"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'DataFrame' object has no attribute 'show'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "could not open socket: [\"tried to connect to ('127.0.0.1', 40911), but an error occurred: [Errno 104] Connection reset by peer\"]"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "The method `pd.Series.__iter__()` is not implemented. If you want to collect your data as an NumPy array, use 'to_numpy()' instead."}
{"is_success": "Fail", "type": "weather_new", "err_msg": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Cannot combine the series or dataframe because it comes from a different dataframe. In order to allow this operation, enable 'compute.ops_on_diff_frames' option."}
{"is_success": "Fail", "type": "weather_new", "err_msg": "cannot concatenate object of type 'list; only ps.Series and ps.DataFrame are valid"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "cannot concatenate object of type 'list; only ps.Series and ps.DataFrame are valid"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "An error occurred while calling o222.pivot"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'list' object has no attribute 'to_spark'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Can not infer schema for type: <class 'str'>"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "name 'sc' is not defined"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "An error occurred while calling o28.json.\n: java.net.ConnectException: Call From localhost/127.0.0.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:779)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 37 more\n"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "\nextraneous input ',' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 2, pos 52)\n\n== SQL ==\nselect DAY, TIME,\n                                                    ,case when PCP = '강수없음' then 0 else PCP end as RAIN\n----------------------------------------------------^^^\n                                                    ,REH as HUMN\n                                                    ,case when SNO = '적설없음' then 0 else SNO end as SNOW\n                                                    ,SKY , TMP as ONDO\n                                                    ,floor((VEC + 22.5 * 0.5) / 22.5) as WINDD\n                                                    ,WSD as WINDS from weather\n"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Column \"PCP\" not found in schema Some(StructType(StructField(W_IDX,DecimalType(38,0),false), StructField(DAY,TimestampType,false), StructField(TIME,StringType,false), StructField(RAIN,DecimalType(38,10),false), StructField(HUMN,DecimalType(38,0),false), StructField(SNOW,DecimalType(38,10),false), StructField(SKY,DecimalType(38,0),false), StructField(ONDO,DecimalType(38,10),false), StructField(WINDD,DecimalType(38,0),true), StructField(WINDS,DecimalType(38,10),true)))"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Column \"PCP\" not found in schema Some(StructType(StructField(W_IDX,DecimalType(38,0),false), StructField(DAY,TimestampType,false), StructField(TIME,StringType,false), StructField(RAIN,DecimalType(38,10),false), StructField(HUMN,DecimalType(38,0),false), StructField(SNOW,DecimalType(38,10),false), StructField(SKY,DecimalType(38,0),false), StructField(ONDO,DecimalType(38,10),false), StructField(WINDD,DecimalType(38,0),true), StructField(WINDS,DecimalType(38,10),true)))"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "name 'cal_std_day_to_date' is not defined"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Column \"PCP\" not found in schema Some(StructType(StructField(W_IDX,DecimalType(38,0),false), StructField(DAY,TimestampType,false), StructField(TIME,StringType,false), StructField(RAIN,DecimalType(38,10),false), StructField(HUMN,DecimalType(38,0),false), StructField(SNOW,DecimalType(38,10),false), StructField(SKY,DecimalType(38,0),false), StructField(ONDO,DecimalType(38,10),false), StructField(WINDD,DecimalType(38,0),true), StructField(WINDS,DecimalType(38,10),true)))"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Column \"PCP\" not found in schema Some(StructType(StructField(W_IDX,DecimalType(38,0),false), StructField(DAY,TimestampType,false), StructField(TIME,StringType,false), StructField(RAIN,DecimalType(38,10),false), StructField(HUMN,DecimalType(38,0),false), StructField(SNOW,DecimalType(38,10),false), StructField(SKY,DecimalType(38,0),false), StructField(ONDO,DecimalType(38,10),false), StructField(WINDD,DecimalType(38,0),true), StructField(WINDS,DecimalType(38,10),true)))"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'DataFrame' object has no attribute 'get_spark_session'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'DataFrame' object has no attribute 'sql'"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "Column \"PCP\" not found in schema Some(StructType(StructField(W_IDX,DecimalType(38,0),false), StructField(DAY,TimestampType,false), StructField(TIME,StringType,false), StructField(RAIN,DecimalType(38,10),false), StructField(HUMN,DecimalType(38,0),false), StructField(SNOW,DecimalType(38,10),false), StructField(SKY,DecimalType(38,0),false), StructField(ONDO,DecimalType(38,10),false), StructField(WINDD,DecimalType(38,0),true), StructField(WINDS,DecimalType(38,10),true)))"}
{"is_success": "Fail", "type": "weather_new", "err_msg": "'DataFrame' object has no attribute 'sql'"}
